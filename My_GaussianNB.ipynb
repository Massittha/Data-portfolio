{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPxxvtBC0reqwF++w38w7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Massittha/Data-portfolio/blob/main/My_GaussianNB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The naive Bayes Classifier\n",
        "\n",
        "## Introduction\n",
        "In this colab notebook, I created my own version of **naive Bayes Classifier** class to make predictions on the breast cancer Wisconsin dataset from `sklearn`, similar to my [previous notebook](https://github.com/Massittha/Data-portfolio/blob/main/My_KNeighbors_Class.ipynb), in which **KNeighbors Classifier** was used to solve the same problem.\n",
        "\n",
        "The steps involved were the following:\n",
        "\n",
        "1. Load the dataset\n",
        "2. Split the data into training and testing sets\n",
        "3. Create my naive Bayes Classifier class\n",
        "4. Predict the training set using my algorithm\n",
        "5. Compare the prediction results with `sklearn GaussianNB`\n",
        "6. Cross validate my algorithm\n"
      ],
      "metadata": {
        "id": "FmHlfQVjm1wB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load the dataset"
      ],
      "metadata": {
        "id": "5ErdLBZvPJLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "Y = cancer.target"
      ],
      "metadata": {
        "id": "h1AT3u4CnSot"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Split the data\n",
        "The dataset was splitted into training and test sets, having the test set of size 0.2 of its original.\n"
      ],
      "metadata": {
        "id": "0_7bnqnKPQZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train , X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state = 10)"
      ],
      "metadata": {
        "id": "_HQNnAFhniPN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. My naive Bayes Classifier class\n",
        "The naive Bayes Classifier assumes all features in the training data are independent of each other, given a class, and values of each feature are normally distributed. The algorithm is constructed with Bayes theorem as follow.\n",
        "$$\n",
        "class_{predicted} = \\text{argmax}_k p(c_k)\\prod_i p(x_i| c_k)\\\\\n",
        "$$\n",
        "\n",
        "where $p(c_k)$ is the probability of the data point being class $c_k$, and $p(x_i|c_k)$ is the probability of a feature $i$ having the value of $x$ given the class $c_k$. The model returns the class $c_k$ that yields the highest probability.\n",
        "\n",
        "By assuming normal distribution of features, the conditional probability can be substituted with the probability distribution function, and applying log to the equation gives:\n",
        "\n",
        "\\begin{align}\n",
        "class_{predicted} &=\\text{argmax}_k (log(p(c_k)) + \\sum_i \\log(p(x_i|c_k)))\\\\\n",
        "&=\\text{arkmax}_k \\left(log(p(c_k)) + \\sum_i \\log\\left(\\frac{1}{\\sigma_k\\sqrt{2\\pi}} exp\\left(\\frac{-(x-\\mu_k)^2}{2\\sigma_k}\\right)\\right)\\right)\\\\\n",
        "\\end{align}\n",
        "\n",
        "\\\n",
        "where $$log\\left(\\frac{1}{\\sigma_k\\sqrt{2\\pi}} exp\\left(\\frac{-(x-\\mu_k)^2}{2\\sigma_k}\\right)\\right)$$ is the **log-likelihood function**\n",
        "\n",
        "\n",
        "In `My_GaussianNB` class below, the `fit` method separetes the training set into a set of class 0 and a set of class 1, and the classification process in the `predict` method is done by applying the Bayes theorem above.\n"
      ],
      "metadata": {
        "id": "akcGDlHvQK2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##The naive Bayes Classifier\n",
        "class My_GaussianNB:\n",
        "\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "    def fit(self,X_train,Y_train):\n",
        "        self.X_train = X_train\n",
        "        self.Y_train = Y_train\n",
        "\n",
        "        ## Separate training data into a set of class 0 and a set of class 1\n",
        "\n",
        "        self.data_0 = X_train[Y_train == 0]\n",
        "        self.data_1 = X_train[Y_train == 1]\n",
        "\n",
        "\n",
        "    def predict(self,X_test):\n",
        "        ## 1. calculate mean and std of each feature in both sets of separated data\n",
        "\n",
        "        means_0 , stds_0 = self.data_0.mean(axis = 0) , self.data_0.std(axis = 0)\n",
        "        means_1 , stds_1 = self.data_1.mean(axis = 0) , self.data_1.std(axis = 0)\n",
        "\n",
        "        ## 2. calculate the probability of the datapoint being each class\n",
        "        ## number of datapoints with each class / Rows of training set\n",
        "\n",
        "        prob_0 = len(self.data_0)/len(self.X_train)\n",
        "        prob_1 = len(self.data_1)/len(self.X_train)\n",
        "\n",
        "\n",
        "        ## 3. calculate log-likelihood\n",
        "        from scipy.stats import norm\n",
        "\n",
        "        log_0 = np.log(prob_0) + np.sum(norm.logpdf(X_test, loc=means_0, scale=stds_0), axis=1)\n",
        "        log_1 = np.log(prob_1) + np.sum(norm.logpdf(X_test, loc=means_1, scale=stds_1), axis=1)\n",
        "\n",
        "\n",
        "        ## 4. prediction\n",
        "\n",
        "        classes = log_0 < log_1\n",
        "\n",
        "        return classes"
      ],
      "metadata": {
        "id": "zr1GirB9pewM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Predict the training set with `My_GaussianNB`"
      ],
      "metadata": {
        "id": "ww8-Mp0SlprG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model = My_GaussianNB()\n",
        "my_model.fit(X_train,Y_train)\n",
        "my_preds = my_model.predict(X_train)"
      ],
      "metadata": {
        "id": "foF500oJlhT3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Compare the results with `sklearn GaussianNB`\n",
        "\n",
        "Both my algorithm and `sklarn GaussianNB` give the same prediction results"
      ],
      "metadata": {
        "id": "qe2oTtZ2l3Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB(var_smoothing=0.)\n",
        "gnb.fit(X_train, Y_train)\n",
        "sk_preds = gnb.predict(X_train)"
      ],
      "metadata": {
        "id": "iCTNr_OHuekV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.all( my_preds == sk_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzHtm8BNunoc",
        "outputId": "17fdda9e-1174-42c7-d854-48c800c6ce72"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Cross-validation\n",
        "KFold cross-validation was applied to validate the generalisability of my algorithm using the training data as the corss-validation input, test data as the unseen set, and number of folds equals to 10."
      ],
      "metadata": {
        "id": "zpPDAAHTn4EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Cross validation\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "kf = KFold(n_splits=10, random_state=10, shuffle=True)\n",
        "\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "iteration = 1\n",
        "    # K-Fold Cross Validation loop\n",
        "\n",
        "for train_index, val_index in kf.split(X_train):\n",
        "        Xtr, Xval = X_train[train_index], X_train[val_index]\n",
        "        ytr, yval = Y_train[train_index], Y_train[val_index]\n",
        "\n",
        "        my_gnb = My_GaussianNB()\n",
        "        my_gnb.fit(Xtr,ytr)\n",
        "        my_gnb.predict(Xval)\n",
        "\n",
        "        pred_train = my_gnb.predict(Xtr)\n",
        "        pred_val = my_gnb.predict(Xval)\n",
        "\n",
        "\n",
        "        # calculate training and validation accuracy and store\n",
        "        train_accuracies.append(accuracy_score(ytr, pred_train))\n",
        "        val_accuracies.append(accuracy_score(yval, pred_val))\n",
        "        print(f\"Iteration {iteration}:\",end = \" \")\n",
        "        print(f\"train accuracy: {round(train_accuracies[iteration-1],4)}, val accuracy: {round(val_accuracies[iteration-1],4)}\")\n",
        "        iteration +=1\n",
        "\n",
        "train_accuracy_mean = np.mean(train_accuracies)\n",
        "val_accuracy_mean = np.mean(val_accuracies)\n",
        "\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Mean Training Accuracy: {train_accuracy_mean:.4f}\")\n",
        "print(f\"Mean Validation Accuracy: {val_accuracy_mean:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5orJOG-Xy9tw",
        "outputId": "4a427adb-090b-4e19-b53c-2ae5e55cb9ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: train accuracy: 0.9193, val accuracy: 1.0\n",
            "Iteration 2: train accuracy: 0.9291, val accuracy: 0.8913\n",
            "Iteration 3: train accuracy: 0.9242, val accuracy: 0.9565\n",
            "Iteration 4: train accuracy: 0.9242, val accuracy: 0.9348\n",
            "Iteration 5: train accuracy: 0.9291, val accuracy: 0.8913\n",
            "Iteration 6: train accuracy: 0.9244, val accuracy: 0.8889\n",
            "Iteration 7: train accuracy: 0.9268, val accuracy: 0.9111\n",
            "Iteration 8: train accuracy: 0.9268, val accuracy: 0.9333\n",
            "Iteration 9: train accuracy: 0.9341, val accuracy: 0.9111\n",
            "Iteration 10: train accuracy: 0.922, val accuracy: 0.9333\n",
            "\n",
            "\n",
            "Mean Training Accuracy: 0.9260\n",
            "Mean Validation Accuracy: 0.9252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the algorithm on the unseen test set"
      ],
      "metadata": {
        "id": "AxY3BjKdpYwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_gnb.fit(X_train,Y_train)\n",
        "my_preds = my_gnb.predict(X_test)\n",
        "\n",
        "print(f\"Final accuracy score: {accuracy_score(Y_test,my_preds):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjlrZo8S0iRk",
        "outputId": "24844445-e13d-4950-f0b2-58180221bd36"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy score: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy scores were similar, hence the model showed generalisability."
      ],
      "metadata": {
        "id": "iMpg406fqEKM"
      }
    }
  ]
}